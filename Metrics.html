

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Metrics &mdash; costcla  documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="costcla  documentation" href="index.html"/>
        <link rel="next" title="Models" href="Models.html"/>
        <link rel="prev" title="Datasets" href="Datasets.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> costcla
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Intro.html">Introduction to Example-Dependent Cost-Sensitive Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="Datasets.html">Datasets</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="">Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="Models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="Probcal.html">Probcal</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sampling.html">Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tutorials.html">CostCla Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Change Log</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">costcla</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          





<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>Metrics</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/Metrics.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-costcla.metrics">
<span id="metrics"></span><h1>Metrics<a class="headerlink" href="#module-costcla.metrics" title="Permalink to this headline">¶</a></h1>
<p>The <a class="reference internal" href="#module-costcla.metrics" title="costcla.metrics"><code class="xref py py-mod docutils literal"><span class="pre">costcla.metrics</span></code></a> module includes metrics to assess performance on cost-sensitive classification tasks given class prediction and cost-matrix</p>
<p>Functions named as <code class="docutils literal"><span class="pre">*_score</span></code> return a scalar value to maximize: the higher the better</p>
<p>Function named as <code class="docutils literal"><span class="pre">*_error</span></code> or <code class="docutils literal"><span class="pre">*_loss</span></code> return a scalar value to minimize: the lower the better</p>
<dl class="function">
<dt id="costcla.metrics.cost_loss">
<code class="descclassname">costcla.metrics.</code><code class="descname">cost_loss</code><span class="sig-paren">(</span><em>y_true</em>, <em>y_pred</em>, <em>cost_mat</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/costcla/metrics/costs.html#cost_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#costcla.metrics.cost_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Cost classification loss.</p>
<p>This function calculates the cost of using y_pred on y_true with
cost-matrix cost-mat. It differ from traditional classification evaluation
measures since measures such as accuracy asing the same cost to different
errors, but that is not the real case in several real-world classification
problems as they are example-dependent cost-sensitive in nature, where the
costs due to misclassification vary between examples.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>y_true</strong> : array-like or label indicator matrix</p>
<blockquote>
<div><p>Ground truth (correct) labels.</p>
</div></blockquote>
<p><strong>y_pred</strong> : array-like or label indicator matrix</p>
<blockquote>
<div><p>Predicted labels, as returned by a classifier.</p>
</div></blockquote>
<p><strong>cost_mat</strong> : array-like of shape = [n_samples, 4]</p>
<blockquote>
<div><p>Cost matrix of the classification problem
Where the columns represents the costs of: false positives, false negatives,
true positives and true negatives, for each example.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>loss</strong> : float</p>
<blockquote class="last">
<div><p>Cost of a using y_pred on y_true with cost-matrix cost-mat</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#costcla.metrics.savings_score" title="costcla.metrics.savings_score"><code class="xref py py-obj docutils literal"><span class="pre">savings_score</span></code></a></p>
</div>
<p class="rubric">References</p>
<table class="docutils citation" frame="void" id="r11" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[R11]</a></td><td>C. Elkan, &#8220;The foundations of Cost-Sensitive Learning&#8221;,
in Seventeenth International Joint Conference on Artificial Intelligence,
973-978, 2001.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="r12" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[R12]</a></td><td>A. Correa Bahnsen, A. Stojanovic, D.Aouada, B, Ottersten,
<a class="reference external" href="http://albahnsen.com/files/%20Improving%20Credit%20Card%20Fraud%20Detection%20by%20using%20Calibrated%20Probabilities%20-%20Publish.pdf">&#8220;Improving Credit Card Fraud Detection with Calibrated Probabilities&#8221;</a>, in Proceedings of the fourteenth SIAM International Conference on Data Mining,
677-685, 2014.</td></tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">costcla.metrics</span> <span class="kn">import</span> <span class="n">cost_loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cost_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cost_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">cost_mat</span><span class="p">)</span>
<span class="go">3</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="costcla.metrics.savings_score">
<code class="descclassname">costcla.metrics.</code><code class="descname">savings_score</code><span class="sig-paren">(</span><em>y_true</em>, <em>y_pred</em>, <em>cost_mat</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/costcla/metrics/costs.html#savings_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#costcla.metrics.savings_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Savings score.</p>
<p>This function calculates the savings cost of using y_pred on y_true with
cost-matrix cost-mat, as the difference of y_pred and the cost_loss of a naive
classification model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>y_true</strong> : array-like or label indicator matrix</p>
<blockquote>
<div><p>Ground truth (correct) labels.</p>
</div></blockquote>
<p><strong>y_pred</strong> : array-like or label indicator matrix</p>
<blockquote>
<div><p>Predicted labels, as returned by a classifier.</p>
</div></blockquote>
<p><strong>cost_mat</strong> : array-like of shape = [n_samples, 4]</p>
<blockquote>
<div><p>Cost matrix of the classification problem
Where the columns represents the costs of: false positives, false negatives,
true positives and true negatives, for each example.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>score</strong> : float</p>
<blockquote class="last">
<div><p>Savings of a using y_pred on y_true with cost-matrix cost-mat</p>
<p>The best performance is 1.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#costcla.metrics.cost_loss" title="costcla.metrics.cost_loss"><code class="xref py py-obj docutils literal"><span class="pre">cost_loss</span></code></a></p>
</div>
<p class="rubric">References</p>
<table class="docutils citation" frame="void" id="r13" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[R13]</a></td><td>A. Correa Bahnsen, A. Stojanovic, D.Aouada, B, Ottersten,
<a class="reference external" href="http://albahnsen.com/files/%20Improving%20Credit%20Card%20Fraud%20Detection%20by%20using%20Calibrated%20Probabilities%20-%20Publish.pdf">&#8220;Improving Credit Card Fraud Detection with Calibrated Probabilities&#8221;</a>, in Proceedings of the fourteenth SIAM International Conference on Data Mining,
677-685, 2014.</td></tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">costcla.metrics</span> <span class="kn">import</span> <span class="n">savings_score</span><span class="p">,</span> <span class="n">cost_loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cost_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">savings_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">cost_mat</span><span class="p">)</span>
<span class="go">0.5</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="costcla.metrics.brier_score_loss">
<code class="descclassname">costcla.metrics.</code><code class="descname">brier_score_loss</code><span class="sig-paren">(</span><em>y_true</em>, <em>y_prob</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/costcla/metrics/costs.html#brier_score_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#costcla.metrics.brier_score_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the Brier score</p>
<p>The smaller the Brier score, the better, hence the naming with &#8220;loss&#8221;.</p>
<p>Across all items in a set N predictions, the Brier score measures the
mean squared difference between (1) the predicted probability assigned
to the possible outcomes for item i, and (2) the actual outcome.
Therefore, the lower the Brier score is for a set of predictions, the
better the predictions are calibrated. Note that the Brier score always
takes on a value between zero and one, since this is the largest
possible difference between a predicted probability (which must be
between zero and one) and the actual outcome (which can take on values
of only 0 and 1).</p>
<p>The Brier score is appropriate for binary and categorical outcomes that
can be structured as true or false, but is inappropriate for ordinal
variables which can take on three or more values (this is because the
Brier score assumes that all possible outcomes are equivalently
&#8220;distant&#8221; from one another).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>y_true</strong> : array, shape (n_samples,)</p>
<p><strong>True targets.</strong></p>
<p><strong>y_prob</strong> : array, shape (n_samples,)</p>
<p><strong>Probabilities of the positive class.</strong></p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>score</strong> : float</p>
<p class="last">Brier score</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/Brier_score">http://en.wikipedia.org/wiki/Brier_score</a></p>
<p class="rubric">Examples</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">costcla.metrics</span> <span class="kn">import</span> <span class="n">brier_score_loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_prob</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">brier_score_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">)</span> 
<span class="go">0.037...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">brier_score_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_prob</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="go">0.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="costcla.metrics.binary_classification_metrics">
<code class="descclassname">costcla.metrics.</code><code class="descname">binary_classification_metrics</code><span class="sig-paren">(</span><em>y_true</em>, <em>y_pred</em>, <em>y_prob</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/costcla/metrics.html#binary_classification_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#costcla.metrics.binary_classification_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>classification_metrics.</p>
<p>This function cal...</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>y_true</strong> : array-like</p>
<blockquote>
<div><p>Ground truth (correct) labels.</p>
</div></blockquote>
<p><strong>y_pred</strong> : array-like</p>
<blockquote>
<div><p>Predicted labels, as returned by a classifier.</p>
</div></blockquote>
<p><strong>y_prob</strong> : array-like</p>
<blockquote>
<div><p>Predicted probabilities, as returned by a classifier.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">dict(tp, fp, fn, tn, accuracy, recall, precision, f1score, auc, brier_loss)</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">costcla.metrics</span> <span class="kn">import</span> <span class="n">binary_classification_metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_prob</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">binary_classification_metrics</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">)</span>
<span class="go">{&#39;accuracy&#39;: 0.75,</span>
<span class="go"> &#39;auc&#39;: 0.75,</span>
<span class="go"> &#39;brier_loss&#39;: 0.13249999999999998,</span>
<span class="go"> &#39;f1score&#39;: 0.6666666666666666,</span>
<span class="go"> &#39;fn&#39;: 1.0,</span>
<span class="go"> &#39;fp&#39;: 0.0,</span>
<span class="go"> &#39;precision&#39;: 1.0,</span>
<span class="go"> &#39;recall&#39;: 0.5,</span>
<span class="go"> &#39;tn&#39;: 2.0,</span>
<span class="go"> &#39;tp&#39;: 1.0}</span>
</pre></div>
</div>
</dd></dl>

</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Models.html" class="btn btn-neutral float-right" title="Models" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Datasets.html" class="btn btn-neutral" title="Datasets" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, Alejandro Correa Bahnsen.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>