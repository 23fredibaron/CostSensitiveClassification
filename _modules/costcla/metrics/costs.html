

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>costcla.metrics.costs &mdash; costcla  documentation</title>
  

  
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  
  
    

  

  
  
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  

  
    <link rel="top" title="costcla  documentation" href="../../../index.html"/>
        <link rel="up" title="costcla.metrics" href="../metrics.html"/> 

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        <a href="../../../index.html" class="fa fa-home"> costcla</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
        
            <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../costcla.datasets.html">costcla.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../costcla.metrics.html">costcla.metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../costcla.models.html">costcla.models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../costcla.models.BayesMinimumRiskClassifier.html">costcla.models.BayesMinimumRiskClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../costcla.models.ThresholdingOptimization.html">costcla.models.ThresholdingOptimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../costcla.models.CostSensitiveLogisticRegression.html">costcla.models.CostSensitiveLogisticRegression</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../costcla.probcal.html">costcla.probcal</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../costcla.sampling.html">costcla.sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../changelog.html">Change Log</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../changelog.html#version-0-04-tbd">Version 0.04 <em>TBD</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../changelog.html#version-0-03-2014-09-19">Version 0.03 <em>(2014-09-19)</em></a></li>
</ul>
</li>
</ul>

        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../../index.html">costcla</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../index.html">Docs</a> &raquo;</li>
      
          <li><a href="../../index.html">Module code</a> &raquo;</li>
      
          <li><a href="../metrics.html">costcla.metrics</a> &raquo;</li>
      
    <li>costcla.metrics.costs</li>
      <li class="wy-breadcrumbs-aside">
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            
  <h1>Source code for costcla.metrics.costs</h1><div class="highlight"><pre>
<span class="sd">&quot;&quot;&quot;Metrics to assess performance on cost-sensitive classification tasks</span>
<span class="sd">given class prediction and cost-matrix</span>

<span class="sd">Functions named as ``*_score`` return a scalar value to maximize: the higher</span>
<span class="sd">the better</span>

<span class="sd">Function named as ``*_error`` or ``*_loss`` return a scalar value to minimize:</span>
<span class="sd">the lower the better</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c"># Authors: Alejandro Correa Bahnsen &lt;al.bahnsen@gmail.com&gt;</span>
<span class="c"># License: BSD 3 clause</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">column_or_1d</span>
<span class="c"># from sklearn.utils import check_consistent_length  # Not in 0.15.1</span>


<div class="viewcode-block" id="cost_loss"><a class="viewcode-back" href="../../../costcla.metrics.html#costcla.metrics.costs.cost_loss">[docs]</a><span class="k">def</span> <span class="nf">cost_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">cost_mat</span><span class="p">):</span>
    <span class="c">#TODO: update description</span>
    <span class="sd">&quot;&quot;&quot;Cost classification loss.</span>

<span class="sd">    This function calculates the cost of using y_pred on y_true with</span>
<span class="sd">    cost-matrix cost-mat. It differ from traditional classification evaluation</span>
<span class="sd">    measures since measures such as accuracy asing the same cost to different</span>
<span class="sd">    errors, but that is not the real case in several real-world classification</span>
<span class="sd">    problems as they are example-dependent cost-sensitive in nature, where the</span>
<span class="sd">    costs due to misclassification vary between examples.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : array-like or label indicator matrix</span>
<span class="sd">        Ground truth (correct) labels.</span>

<span class="sd">    y_pred : array-like or label indicator matrix</span>
<span class="sd">        Predicted labels, as returned by a classifier.</span>

<span class="sd">    cost_mat : array-like of shape = [n_samples, 4]</span>
<span class="sd">        Cost matrix of the classification problem</span>
<span class="sd">        Where the columns represents the costs of: false positives, false negatives,</span>
<span class="sd">        true positives and true negatives, for each example.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss : float</span>
<span class="sd">        Cost of a using y_pred on y_true with cost-matrix cost-mat</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] C. Elkan, &quot;The foundations of Cost-Sensitive Learning&quot;,</span>
<span class="sd">           in Seventeenth International Joint Conference on Artificial Intelligence,</span>
<span class="sd">           973-978, 2001.</span>

<span class="sd">    .. [2] A. Correa Bahnsen, A. Stojanovic, D.Aouada, B, Ottersten,</span>
<span class="sd">           `&quot;Improving Credit Card Fraud Detection with Calibrated Probabilities&quot; &lt;http://albahnsen.com/files/%20Improving%20Credit%20Card%20Fraud%20Detection%20by%20using%20Calibrated%20Probabilities%20-%20Publish.pdf&gt;`__, in Proceedings of the fourteenth SIAM International Conference on Data Mining,</span>
<span class="sd">           677-685, 2014.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    savings_score</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from costcla.metrics import cost_loss</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [0, 1, 0, 0]</span>
<span class="sd">    &gt;&gt;&gt; y_true = [0, 1, 1, 0]</span>
<span class="sd">    &gt;&gt;&gt; cost_mat = np.array([[4, 1, 0, 0], [1, 3, 0, 0], [2, 3, 0, 0], [2, 1, 0, 0]])</span>
<span class="sd">    &gt;&gt;&gt; cost_loss(y_true, y_pred, cost_mat)</span>
<span class="sd">    3</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c">#TODO: Check consistency of cost_mat</span>

    <span class="n">y_true</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">*</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">*</span> <span class="n">cost_mat</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">y_pred</span> <span class="o">*</span> <span class="n">cost_mat</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">cost</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">*</span> <span class="n">cost_mat</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">*</span> <span class="n">cost_mat</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

</div>
<div class="viewcode-block" id="savings_score"><a class="viewcode-back" href="../../../costcla.metrics.html#costcla.metrics.costs.savings_score">[docs]</a><span class="k">def</span> <span class="nf">savings_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">cost_mat</span><span class="p">):</span>
    <span class="c">#TODO: update description</span>
    <span class="sd">&quot;&quot;&quot;Savings score.</span>

<span class="sd">    This function calculates the savings cost of using y_pred on y_true with</span>
<span class="sd">    cost-matrix cost-mat, as the difference of y_pred and the cost_loss of a naive</span>
<span class="sd">    classification model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : array-like or label indicator matrix</span>
<span class="sd">        Ground truth (correct) labels.</span>

<span class="sd">    y_pred : array-like or label indicator matrix</span>
<span class="sd">        Predicted labels, as returned by a classifier.</span>

<span class="sd">    cost_mat : array-like of shape = [n_samples, 4]</span>
<span class="sd">        Cost matrix of the classification problem</span>
<span class="sd">        Where the columns represents the costs of: false positives, false negatives,</span>
<span class="sd">        true positives and true negatives, for each example.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    score : float</span>
<span class="sd">        Savings of a using y_pred on y_true with cost-matrix cost-mat</span>

<span class="sd">        The best performance is 1.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] A. Correa Bahnsen, A. Stojanovic, D.Aouada, B, Ottersten,</span>
<span class="sd">           `&quot;Improving Credit Card Fraud Detection with Calibrated Probabilities&quot; &lt;http://albahnsen.com/files/%20Improving%20Credit%20Card%20Fraud%20Detection%20by%20using%20Calibrated%20Probabilities%20-%20Publish.pdf&gt;`__, in Proceedings of the fourteenth SIAM International Conference on Data Mining,</span>
<span class="sd">           677-685, 2014.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    cost_loss</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from costcla.metrics import savings_score, cost_loss</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [0, 1, 0, 0]</span>
<span class="sd">    &gt;&gt;&gt; y_true = [0, 1, 1, 0]</span>
<span class="sd">    &gt;&gt;&gt; cost_mat = np.array([[4, 1, 0, 0], [1, 3, 0, 0], [2, 3, 0, 0], [2, 1, 0, 0]])</span>
<span class="sd">    &gt;&gt;&gt; savings_score(y_true, y_pred, cost_mat)</span>
<span class="sd">    0.5</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c">#TODO: Check consistency of cost_mat</span>
    <span class="n">y_true</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>

    <span class="c"># Calculate the cost of naive prediction</span>
    <span class="n">cost_base</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">cost_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_samples</span><span class="p">),</span> <span class="n">cost_mat</span><span class="p">),</span>
                    <span class="n">cost_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_samples</span><span class="p">),</span> <span class="n">cost_mat</span><span class="p">))</span>

    <span class="n">cost</span> <span class="o">=</span> <span class="n">cost_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">cost_mat</span><span class="p">)</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">cost</span> <span class="o">/</span> <span class="n">cost_base</span>


<span class="c"># from https://github.com/agramfort/scikit-learn/blob/isotonic_calibration/sklearn/metrics/classification.py</span>
<span class="c"># Waiting for https://github.com/scikit-learn/scikit-learn/pull/1176</span>
<span class="c">#TODO: Remove when #1176 is merged</span></div>
<div class="viewcode-block" id="brier_score_loss"><a class="viewcode-back" href="../../../costcla.metrics.html#costcla.metrics.costs.brier_score_loss">[docs]</a><span class="k">def</span> <span class="nf">brier_score_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the Brier score</span>

<span class="sd">    The smaller the Brier score, the better, hence the naming with &quot;loss&quot;.</span>

<span class="sd">    Across all items in a set N predictions, the Brier score measures the</span>
<span class="sd">    mean squared difference between (1) the predicted probability assigned</span>
<span class="sd">    to the possible outcomes for item i, and (2) the actual outcome.</span>
<span class="sd">    Therefore, the lower the Brier score is for a set of predictions, the</span>
<span class="sd">    better the predictions are calibrated. Note that the Brier score always</span>
<span class="sd">    takes on a value between zero and one, since this is the largest</span>
<span class="sd">    possible difference between a predicted probability (which must be</span>
<span class="sd">    between zero and one) and the actual outcome (which can take on values</span>
<span class="sd">    of only 0 and 1).</span>

<span class="sd">    The Brier score is appropriate for binary and categorical outcomes that</span>
<span class="sd">    can be structured as true or false, but is inappropriate for ordinal</span>
<span class="sd">    variables which can take on three or more values (this is because the</span>
<span class="sd">    Brier score assumes that all possible outcomes are equivalently</span>
<span class="sd">    &quot;distant&quot; from one another).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : array, shape (n_samples,)</span>
<span class="sd">    True targets.</span>

<span class="sd">    y_prob : array, shape (n_samples,)</span>
<span class="sd">    Probabilities of the positive class.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    score : float</span>
<span class="sd">    Brier score</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from costcla.metrics import brier_score_loss</span>
<span class="sd">    &gt;&gt;&gt; y_true = [0, 1, 1, 0]</span>
<span class="sd">    &gt;&gt;&gt; y_prob = [0.1, 0.9, 0.8, 0.3]</span>
<span class="sd">    &gt;&gt;&gt; brier_score_loss(y_true, y_prob) # doctest: +ELLIPSIS</span>
<span class="sd">    0.037...</span>
<span class="sd">    &gt;&gt;&gt; brier_score_loss(y_true, np.array(y_prob) &gt; 0.5)</span>
<span class="sd">    0.0</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    http://en.wikipedia.org/wiki/Brier_score</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_true</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="n">y_prob</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y_prob</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_prob</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span></div>
</pre></div>

          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2014, Alejandro Correa Bahnsen.
    </p>
  </div>

  <a href="https://github.com/snide/sphinx_rtd_theme">Sphinx theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>
</footer>
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>