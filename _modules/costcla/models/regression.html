

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>costcla.models.regression &mdash; costcla  documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="costcla  documentation" href="../../../index.html"/>
        <link rel="up" title="Module code" href="../../index.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        

        
          <a href="../../../index.html" class="icon icon-home"> costcla
        

        
        </a>

        
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

        
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
          
          
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Datasets.html">Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Metrics.html">Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Models.html">Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../BayesMinimumRiskClassifier.html">BayesMinimumRiskClassifier</a><ul class="simple">
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../ThresholdingOptimization.html">ThresholdingOptimization</a><ul class="simple">
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../CostSensitiveLogisticRegression.html">CostSensitiveLogisticRegression</a><ul class="simple">
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../CostSensitiveDecisionTreeClassifier.html">CostSensitiveDecisionTreeClassifier</a><ul class="simple">
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../CostSensitiveRandomForestClassifier.html">CostSensitiveRandomForestClassifier</a><ul class="simple">
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../CostSensitiveBaggingClassifier.html">CostSensitiveBaggingClassifier</a><ul class="simple">
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../CostSensitivePastingClassifier.html">CostSensitivePastingClassifier</a><ul class="simple">
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../CostSensitiveRandomPatchesClassifier.html">CostSensitiveRandomPatchesClassifier</a><ul class="simple">
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../Probcal.html">Probcal</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../Sampling.html">Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Tutorials.html">CostCla Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../Tutorials.html#tutorial-example-dependent-cost-sensitive-credit-scoring-using-costcla">Tutorial: Example-Dependent Cost-Sensitive Credit Scoring using CostCla</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../changelog.html">Change Log</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../changelog.html#version-0-4-2015-05-26">Version 0.4 <em>(2015-05-26)</em></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../changelog.html#version-0-03-2014-09-19">Version 0.03 <em>(2014-09-19)</em></a></li>
</ul>
</li>
</ul>

          
        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../../index.html">costcla</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../index.html">Docs</a> &raquo;</li>
      
          <li><a href="../../index.html">Module code</a> &raquo;</li>
      
    <li>costcla.models.regression</li>
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document">
            
  <h1>Source code for costcla.models.regression</h1><div class="highlight"><pre>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">This module include the cost-sensitive logistic regression method.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c"># Authors: Alejandro Correa Bahnsen &lt;al.bahnsen@gmail.com&gt;</span>
<span class="c"># License: BSD 3 clause</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span>
<span class="c"># from sklearn.linear_model.logistic import _intercept_dot</span>
<span class="kn">from</span> <span class="nn">pyea</span> <span class="kn">import</span> <span class="n">GeneticAlgorithmOptimizer</span>
<span class="kn">from</span> <span class="nn">..metrics</span> <span class="kn">import</span> <span class="n">cost_loss</span>

<span class="c"># Not in sklearn 0.15, is in 0.16-git</span>
<span class="c">#TODO: replace once sklearn 0.16 is release</span>
<span class="c"># The one in sklearn 0.16 return yz instead of z, therefore,</span>
<span class="c"># the impact on the code should be addressed before making the change.</span>
<span class="k">def</span> <span class="nf">_intercept_dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes y * np.dot(X, w).</span>

<span class="sd">    It takes into consideration if the intercept should be fit or not.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    w : ndarray, shape (n_features,) or (n_features + 1,)</span>
<span class="sd">        Coefficient vector.</span>

<span class="sd">    X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">c</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">if</span> <span class="n">w</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">z</span>


<span class="k">def</span> <span class="nf">_sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Private function that calculate the sigmoid function &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_logistic_cost_loss_i</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cost_mat</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">w</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">_intercept_dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">y_prob</span> <span class="o">=</span> <span class="n">_sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">cost_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">,</span> <span class="n">cost_mat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span>
    <span class="n">out</span> <span class="o">+=</span> <span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">_logistic_cost_loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cost_mat</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the logistic loss.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    w : array-like, shape (n_w, n_features,) or (n_w, n_features + 1,)</span>
<span class="sd">        Coefficient vector or matrix of coefficient.</span>

<span class="sd">    X : array-like, shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    y : ndarray, shape (n_samples,)</span>
<span class="sd">        Array of labels.</span>

<span class="sd">    cost_mat : array-like of shape = [n_samples, 4]</span>
<span class="sd">        Cost matrix of the classification problem</span>
<span class="sd">        Where the columns represents the costs of: false positives, false negatives,</span>
<span class="sd">        true positives and true negatives, for each example.</span>

<span class="sd">    alpha : float</span>
<span class="sd">        Regularization parameter. alpha is equal to 1 / C.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : float</span>
<span class="sd">        Logistic loss.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">w</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="c"># Only evaluating one w</span>
        <span class="k">return</span> <span class="n">_logistic_cost_loss_i</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cost_mat</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="c"># Evaluating a set of w</span>
        <span class="n">n_w</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_w</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_w</span><span class="p">):</span>
            <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">_logistic_cost_loss_i</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cost_mat</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>


<div class="viewcode-block" id="CostSensitiveLogisticRegression"><a class="viewcode-back" href="../../../CostSensitiveLogisticRegression.html#costcla.models.CostSensitiveLogisticRegression">[docs]</a><span class="k">class</span> <span class="nc">CostSensitiveLogisticRegression</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A example-dependent cost-sensitive Logistic Regression classifier.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    C : float, optional (default=1.0)</span>
<span class="sd">        Inverse of regularization strength; must be a positive float.</span>
<span class="sd">        Like in support vector machines, smaller values specify stronger</span>
<span class="sd">        regularization.</span>

<span class="sd">    fit_intercept : bool, default: True</span>
<span class="sd">        Specifies if a constant (a.k.a. bias or intercept) should be</span>
<span class="sd">        added the decision function.</span>

<span class="sd">    max_iter : int</span>
<span class="sd">        Useful only for the ga and bfgs solvers. Maximum number of</span>
<span class="sd">        iterations taken for the solvers to converge.</span>

<span class="sd">    random_state : int seed, RandomState instance, or None (default)</span>
<span class="sd">        The seed of the pseudo random number generator to use when</span>
<span class="sd">        shuffling the data.</span>

<span class="sd">    solver : {&#39;ga&#39;, &#39;bfgs&#39;}</span>
<span class="sd">        Algorithm to use in the optimization problem.</span>

<span class="sd">    tol : float, optional</span>
<span class="sd">        Tolerance for stopping criteria.</span>

<span class="sd">    verbose : int, optional (default=0)</span>
<span class="sd">        Controls the verbosity of the optimization process.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    `coef_` : array, shape (n_classes, n_features)</span>
<span class="sd">        Coefficient of the features in the decision function.</span>

<span class="sd">    `intercept_` : array, shape (n_classes,)</span>
<span class="sd">        Intercept (a.k.a. bias) added to the decision function.</span>
<span class="sd">        If `fit_intercept` is set to False, the intercept is set to zero.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    sklearn.tree.DecisionTreeClassifier</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>

<span class="sd">    .. [1] A. Correa Bahnsen, D.Aouada, B, Ottersten,</span>
<span class="sd">           `&quot;Example-Dependent Cost-Sensitive Logistic Regression for Credit Scoring&quot; &lt;http://albahnsen.com/files/Example-Dependent%20Cost-Sensitive%20Logistic%20Regression%20for%20Credit%20Scoring_publish.pdf&gt;`__,</span>
<span class="sd">           in Proceedings of the International Conference on Machine Learning and Applications,</span>
<span class="sd">           , 2014.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.linear_model import LogisticRegression</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.cross_validation import train_test_split</span>
<span class="sd">    &gt;&gt;&gt; from costcla.datasets import load_creditscoring2</span>
<span class="sd">    &gt;&gt;&gt; from costcla.models import CostSensitiveLogisticRegression</span>
<span class="sd">    &gt;&gt;&gt; from costcla.metrics import savings_score</span>
<span class="sd">    &gt;&gt;&gt; data = load_creditscoring2()</span>
<span class="sd">    &gt;&gt;&gt; sets = train_test_split(data.data, data.target, data.cost_mat, test_size=0.33, random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; X_train, X_test, y_train, y_test, cost_mat_train, cost_mat_test = sets</span>
<span class="sd">    &gt;&gt;&gt; y_pred_test_lr = LogisticRegression(random_state=0).fit(X_train, y_train).predict(X_test)</span>
<span class="sd">    &gt;&gt;&gt; f = CostSensitiveLogisticRegression()</span>
<span class="sd">    &gt;&gt;&gt; f.fit(X_train, y_train, cost_mat_train)</span>
<span class="sd">    &gt;&gt;&gt; y_pred_test_cslr = f.predict(X_test)</span>
<span class="sd">    &gt;&gt;&gt; # Savings using Logistic Regression</span>
<span class="sd">    &gt;&gt;&gt; print savings_score(y_test, y_pred_test_lr, cost_mat_test)</span>
<span class="sd">    0.00283419465107</span>
<span class="sd">    &gt;&gt;&gt; # Savings using Cost Sensitive Logistic Regression</span>
<span class="sd">    &gt;&gt;&gt; print savings_score(y_test, y_pred_test_cslr, cost_mat_test)</span>
<span class="sd">    0.142872237978</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                 <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">solver</span><span class="o">=</span><span class="s">&#39;ga&#39;</span><span class="p">,</span>
                 <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">C</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">fit_intercept</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="o">=</span> <span class="n">solver</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="mf">0.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

<div class="viewcode-block" id="CostSensitiveLogisticRegression.fit"><a class="viewcode-back" href="../../../CostSensitiveLogisticRegression.html#costcla.models.CostSensitiveLogisticRegression.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cost_mat</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Build a example-dependent cost-sensitive logistic regression from the training set (X, y, cost_mat)</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape = [n_samples, n_features]</span>
<span class="sd">            The input samples.</span>

<span class="sd">        y : array indicator matrix</span>
<span class="sd">            Ground truth (correct) labels.</span>

<span class="sd">        cost_mat : array-like of shape = [n_samples, 4]</span>
<span class="sd">            Cost matrix of the classification problem</span>
<span class="sd">            Where the columns represents the costs of: false positives, false negatives,</span>
<span class="sd">            true positives and true negatives, for each example.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns self.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c">#TODO: Check input</span>

        <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">:</span>
            <span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="o">==</span> <span class="s">&#39;ga&#39;</span><span class="p">:</span>
            <span class="c">#TODO: add n_jobs</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">GeneticAlgorithmOptimizer</span><span class="p">(</span><span class="n">_logistic_cost_loss</span><span class="p">,</span>
                                            <span class="n">w0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                            <span class="n">iters</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span>
                                            <span class="n">type_</span><span class="o">=</span><span class="s">&#39;cont&#39;</span><span class="p">,</span>
                                            <span class="n">n_chromosomes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                            <span class="n">per_mutations</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
                                            <span class="n">n_elite</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                            <span class="n">fargs</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cost_mat</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">),</span>
                                            <span class="n">range_</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                                            <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                            <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
            <span class="n">res</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="o">==</span> <span class="s">&#39;bfgs&#39;</span><span class="p">:</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">disp</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">disp</span> <span class="o">=</span> <span class="bp">False</span>

            <span class="n">res</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">_logistic_cost_loss</span><span class="p">,</span>
                           <span class="n">w0</span><span class="p">,</span>
                           <span class="n">method</span><span class="o">=</span><span class="s">&#39;BFGS&#39;</span><span class="p">,</span>
                           <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cost_mat</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">),</span>
                           <span class="n">tol</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
                           <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;maxiter&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span> <span class="s">&#39;disp&#39;</span><span class="p">:</span> <span class="n">disp</span><span class="p">})</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span>
</div>
<div class="viewcode-block" id="CostSensitiveLogisticRegression.predict_proba"><a class="viewcode-back" href="../../../CostSensitiveLogisticRegression.html#costcla.models.CostSensitiveLogisticRegression.predict_proba">[docs]</a>    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Probability estimates.</span>

<span class="sd">        The returned estimates.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape = [n_samples, n_features]</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        T : array-like, shape = [n_samples, 2]</span>
<span class="sd">            Returns the probability of the sample for each class in the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">y_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">y_prob</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">_sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
        <span class="n">y_prob</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">y_prob</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">y_prob</span>
</div>
<div class="viewcode-block" id="CostSensitiveLogisticRegression.predict"><a class="viewcode-back" href="../../../CostSensitiveLogisticRegression.html#costcla.models.CostSensitiveLogisticRegression.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">cut_point</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predicted class.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape = [n_samples, n_features]</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        T : array-like, shape = [n_samples]</span>
<span class="sd">            Returns the prediction of the sample..</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">cut_point</span><span class="p">))</span></div></div>
</pre></div>

          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2014, Alejandro Correa Bahnsen.
    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>